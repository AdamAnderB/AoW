\section{2. Replication of \textcite{Porretta_et_al_2020}}

\subsection{2.1 background}

We carried out a single change replication study \parencite{Marsden_2018} involving web-based data collection of \textcite{Porretta_et_al_2020}'s in-person VWP experiment, which found that a foreign accent impedes predictive processing but does not preclude it. \textcite{Porretta_et_al_2020} was chosen for replication for three principled reasons following the recommendations of \textcite{Marsden_2018}. First, the initial studies' transparency minimizes heterogeneity. That is, the majority of materials were made available by the researchers. Secondly, the recency and novelty of the initial study and its findings warrant replication for the sake of validation and generalizability. Lastly, the cost of eye-tracking studies has historically unincentivized replications. If successful, this study would be meaningful for future replications and for accessibility to eye-tracking in general.

\textcite{Porretta_et_al_2020} used a standard 2x2 VWP design discussed above to explore predictive sentence processing with a 2 by 2 experimental design testing talker and verb type. Here, verb type refers to two types of transitive sentence: restrictive (e.g., “the fireman will climb the ladder”, \textit{climb} allows for object prediction) or unrestrictive (e.g., “the fireman will need the ladder”, \textit{need} does not allow for object prediction). Additionally, these sentences were spoken by either a native or Chinese accented English speaker (talker).

Whereas our study changed only the method of collecting data from in-person to online. This single change causes three immediate differences between our replication and \parencite{Porretta_et_al_2020} summarized in table\ref{table:replication}. Additionally, as the reader will see below, this single change leads to a variety of differences that required explicit choices to be made throughout data analysis.

\begin{table}[]
\label{table:replication}
\begin{tabular}{ccc}
               & \textcite{Porretta_et_al_2020}     & Our web-based replication   \\ \hline
Eye-tracker    & Eyelink 1000        & Variable personal webcams                      \\
Participants   & 60 university students & 60 prolific population \\
Data wrangling & pre-processed       & self wrangled                    \\ \hline
\end{tabular}
\end{table}


\subsection{2.2 Methods}

 To replicate \textcite{Porretta_et_al_2020}, we used Gorilla Experiment Builder’s eye-tracking 2 zone implemented with WebGazer.js \parencite{Anwyl-Irvine_2019, Papoutsaki}. All research materials, R data analysis, Gorilla experiment and tasks, and data are available on the Open Science Framework (OSF) \parencite{OSF}. The study was approved by the authors’ Institutional Review Board. All participants were compensated for their participation. Average completion time of the experiment was 16 minutes including a second task that is not reported here.

 \subsection{2.2.1 Participants}
 To keep the study as consistent as possible with \textcite{Porretta_et_al_2020}, we capped participation in our study at 60 participants. Additionally, within Prolific we required that the participants were native monolingual English speakers, between the ages of 18 to 40. Recruitment of 60 participants for the experiment was managed through Prolific \parencite{Palan_2018} and linked to Gorilla. 37 participants were rejected from participation based on not meeting initial calibration requirements; including: 8 headphone check failures \parencite{milne_2021}, 23 eye-calibration failures, 5 time outs 90-minutee limit), 1 IRB non-consent.   

\subsection{2.2.2 Materials}
In our replication, all recordings were taken from \textcite{Porretta_et_al_2020} and speaker was counterbalanced across four lists. The experiment contains 250 images, 50 of which are center images and 200 that make up the 2x2 design (10 practice, 120 filler, 120 experimental). 99 of the images were identical to the original experiment (all 50 center images for subjects in the sentences and 49 of the visual stimuli for objects across practice, filler, and experimental items). The remaining 151 images were obtained following the same specifications of the initial study (open source line-drawn images). Four of the images were created by lab members in-house due to not being available online. 

\subsection{2.2.3 Procedure}

Participants were recruited through Prolific and logged on to Gorilla on their own with their own personal computers. After consenting to participate, each participant went through a series of calibration checks. Each participant needed to agree to using headphones and having their eyes tracked throughout the experiment. Each participant did two headphone checks. The first is a simple sound check to ensure that they can hear, which plays oral instructions to guide them on how to continue to the next page. This is a basic threshold for ensuring that they have audio on loud enough. Next, the participants each went through a dichotic pitch task to ensure that their headphones or speakers were of sufficient quality to participate. Next, participants did a 5-point eye-calibration set to reject participants below 4 calibration success with a limit of 3 calibration attempts before rejection. On each trial (24 target, 24 filler), participants were presented with a 500-ms fixation cross followed by a 2x2 visual stimulus with an additional center image that represented the subject of the sentence. Each stimulus was previewed for 200ms. Next, participants heard either a restrictive or nonrestrictive sentence spoken with either a native accent or non-native accent. Participants then answered a simple comprehension question to ensure attention. After the experimental task, Participants then filled out a brief background questionnaire. 

\subsection{2.3 Data Analysis}

Here, we will report in detail how the data is wrangled for the statistical analyses and visualizations. Detailed information on statistical analyses, visualizations, as well as results will be reported at the end of Act 3. In what follows, "L: + line number" (e.g., L: 156-157) refer to line numbers in \inlineR{AOW\_r\_work\_flow.rmd} found on OSF. In L: 33, we read in three data frames: The \inlineR{task\_data}, \inlineR{eye\_tracking\_data}, and \inlineR{OSF\_data}. To run this, download the data folder from OSF and select task\_data.csv when prompted by R after running L: 33. You can load the other data frames by running the following lines. The \inlineR{task\_data} is made up of the experimental data and information obtained during testing. This looks similar to the data in the \inlineR{task\_data} of figure \ref{fig:data_structure}. \inlineR{eye\_tracking\_data} is made up of eye-fixations (like figure \ref{fig:data_structure}). \inlineR{task\_data} is made up of a messy 97,827 rows by 111 columns. And, \inlineR{eye\_tracking\_data} is made up of an overwhelming 400,305 rows by 36 columns. As mentioned above, The data are relational. Combining takes both time and wrangling. In the next 200 lines of code, we wrangle these structures into useful data that we can fully use, adapt, and share. See supplementary materials for methods on combining separate experimental files into a single data frame.

\lstinputlisting[style=mystyle,firstnumber=31]{scripts/chunk-Data Reading.R}

\subsubsection{Questionnaire wrangling}
Data wrangling always starts with data removal. In a VWP experiment, removal occurs at four levels (questionnaire-based, item-based, behavior-based, fixation-quality-based). While the exact point you start is unimportant, here, we start with simple first (questionnaire-based removal): Which participants should be excluded based on questionnaire exclusion criteria  (e.g., not an L1 English speaker and not between the ages of 18 and 40)? In line 43, we start with a clone of our behavioral data frame \inlineR{task\_data} and assess needed variables (\inlineR{Screen.Name}, \inlineR{Responses}, \inlineR{Participant.Private.ID}, \inlineR{Reaction.Time}(RT)). RT is kept here because it allows for removing items that were unnecessarily generated from the experiment structure (i.e., getting rid of rows with 0 RT). 

\lstinputlisting[style=mystyle,firstnumber=42]{scripts/chunk-Questionnaire: Clean.R}

Now that we have a data frame with three columns (\inlineR{Participant.Private.ID}, \inlineR{Screen.Name}, \inlineR{Response}), we can restructure the data into tidy data (one observation per row and one variable per column) \parencite{Wickham_2014}. \inlineR{pivot\_wider()} and \inlineR{pivot\_longer()} offer a simple solution to this common data structure problem, see figure \ref{fig:pivoting}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/pivoting.png}
    \caption{Note: Pivoting Wider and Pivoting longer are both useful in creating tidy data in different situations. Experimental data (e.g., Gorilla-tasks, Psychopy, E-Prime) often requires widening, whereas questionnaire data (e.g., Gorilla-questionnaires, Google forms, Qualtrix) need to be pivoted longer.}
    \label{fig:pivoting}
    
\end{figure}

Starting at line 49, we pivot wider to create a single row for each participant with each question having its own column. One way to understand the importance of pivoting is by realizing that it is much easier to come up with standards for removal in the \inlineR{speaks\_L2}, \inlineR{age}, or \inlineR{hear\_impaired} columns than for the \inlineR{Response} column, which would require conditional standards based on \inlineR{Screen.Name}.

\lstinputlisting[style=mystyle, firstnumber=49]{scripts/chunk-Questionnaire: Tidy.R}

Starting at line 69, we find that two participants should be removed for language expertise outside English and one for exceeding the age cut off (both predetermined values). We can filter this data to remove the participants that do not qualify. See lines between these data chunks in \inlineR{AOW\_r\_work\_flow.rmd} for an example of visualizations created through specific data type (e.g., visualizing all numeric columns). 

\lstinputlisting[style=mystyle,
firstnumber=69]{scripts/chunk-Questionnaire: Filtered.R}

We can now answer our question: Which participants should be excluded based on the background questionnaire? We use this data frame to filter out unqualified participants in the \inlineR{Participant.Private.ID} column of the next removal stage.


\subsubsection{Behavioral-task wrangling}

The next cycle of data wrangling begins with the question: Which participants and items should be removed based on the behavioral results of the experiment? Cleaning is similar to the questionnaire cycle, but we start from scratch with a clone of \inlineR{task\_data} called \inlineR{experimental\_cleaned} because the new question has new goals, which requires different variables. We start this cycle's implementation by filtering the participants in the behavioral-task clone with the questionnaire data from above in order to only keep those participants that qualified in the questionnaire wrangling cycle (L: 77). We then remove all rows except ones relating to behavioral data questions (L: 78-79) and experimental items (L: 80), followed by removing columns with all NAs. Lastly, to achieve tidy data, we split the visual image selection and comprehension question into two columns so that each participant has a single observation for each trial (e.g., pivot into a wider structure (L: 84)). Removal of columns in L: 86-88 makes pivoting possible. Pivoting requires that rows do not have uniquely identifiable information outside the data columns being "widened" (This could also be achieved with the column argument of \inlineR{pivot\_wider}). 

\lstinputlisting[style=mystyle, firstnumber=76]{scripts/chunk-Experimental Data: Clean and Tidy.R}

Additionally, we must load in a second data frame \inlineR{OSF\_data} (L: 94) from the original experiment. We do this for two reasons: 1) Because our experiment only has the quadrants or the visual stimuli without the target, competitor, and distractor information, and 2) our experiment does not have \inlineR{SUBTLWF\_obj}, which is the log frequency of the object words needed much later in the statistical models.

\lstinputlisting[style=mystyle, firstnumber=93]{scripts/chunk-OSF Data: Clean and Tidy.R}

In L: 99, we filter the \inlineR{OSF\_data} for experimental items and use a \inlineR{left\_join()} based on \inlineR{talker}, condition \inlineR{verb\_type}, and the center visual image \inlineR{subject\_img\_file}, which simultaneously pulls in the variables that we need and filters out nonce items (this step could be avoided by putting these variables in the original experimental spreadsheets). See figure \ref{fig:joins} for a visualization of filtering through different types of joining.

\lstinputlisting[style=mystyle, firstnumber=98]{scripts/chunk-Behavioral Data: Join OSF and Experimental Data.R}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/joins.png}
    \caption{Joins act as filters, determining what data to include or exclude based on commonalities and differences between data frames. Solid portions refer to what is kept.}
    \label{fig:joins}
\end{figure}

Now that we have the variables we need from joining the datasets to create \inlineR{behavioral\_data} using \inlineR{left\_join()}, we can create variables for the answers being correct and incorrect for our removal process. We will do this for both the item selection (L: 105) and the following comprehension question (L: 106).

\lstinputlisting[style=mystyle, firstnumber=102]{scripts/chunk-Behavioral Data: Clean and Tidy.R}

Importantly, researchers should establish a criteria for removal prior to data collection. Because the authors did not report the criteria they used, we based our removal on three standard deviations from the mean inaccuracy of participants/items separately.  

\lstinputlisting[style=mystyle, firstnumber=108]{scripts/chunk-Behavioral Data: Removal Standards.R}

 We implemented these standards through a series of modular aggregating steps. That is, we aggregated the inaccuracies of participants by simply adding together incorrect items by participant and item for both item selection (L: 118-129) and comprehension question (L: 131-142), respectively. Then in line 144, individuals incorrect trials were removed. We end here by removing the incorrect trials to prepare for the eye-tracking data wrangling (L: 144-145).

\lstinputlisting[style=mystyle, firstnumber=116]{scripts/chunk-Behavioral Data: Participant and Item Removal.R}

One important note about this removal is that the removal is done in parallel. That is, we removed participants and items simultaneously. If you sequentially remove participant or item first then removal results would be different in the \inlineR{behaviorial\_data}

\subsubsection{Eye-tracking Wrangling}

Removal and adjustment of eye-tracking data is done through an exploratory lens as there is little current reference for expected results for eye-fixations and frame rate in web-based eye-tracking. However, recent work has begun to fill this gap \parencite[see:][, for other pioneering work in web-based eye-tracking]{Vos_2017,Prystauka_Altmann_Rothman_2023}. Here, two questions guide our approach: How should eye-fixations be classified into quadrants in web-based eye-tracking? And, what quality of frame rate is needed to capture the effects of interest? We start by filtering out participants from the previous data sets. Here, the retained participants (L: 118) and items (L: 131) from the previous step are used to define what we want to keep in the \inlineR{behavioral\_data} (L: 148-150) with the \inlineR{\%in\%} operator.

\lstinputlisting[style=mystyle, firstnumber=147]{scripts/chunk-Behavioral Data: Removing with IN Operator.R}

Whereas the \inlineR{et\_data} is much larger than the previous data frames, the same methods are used. Selection of data can be reduced to only the time \inlineR{time\_elapsed}, participant \inlineR{participant\_id}, and eye-fixations \inlineR{x\_pred\_normalised} \inlineR{y\_pred\_normalised} (L: 154-156), which is filtered by only usable fixation points (L: 157), followed by variable renaming for upcoming joining of \inlineR{et\_data} and \inlineR{behavioral\_data} (L: 158-159).

\lstinputlisting[style=mystyle, firstnumber=153]{scripts/chunk-ET Data: Tidying and Filtering with an Inner Join.R}

Now that both \inlineR{behavioral\_data} and \inlineR{et\_data} are cleaned and tidy. \inlineR{left\_join()} (L: 173) is used to create \inlineR{all\_data} from our \inlineR{behavioral\_data} and \inlineR{eye\_tracking} data. This data frame now has all of the eye-tracking data and behavioral-task data from the entire experiment (L: 173-174). However, the data from the \inlineR{et\_data} only includes unclassified eye-fixations. Specifically, it includes the x and y coordinates without a link to the visual stimuli that are being viewed (see figure \ref{fig:core_four} for a review). A Shiny app was created to dynamically explore how eye-fixations are distributed with variable amounts of removal at four crucial time points: the beginning of the sentence (-400 ms), verb onset (0 ms), object onset, and selection of visual stimuli. The app also includes dynamically calculated data loss. Figure \ref{fig:signal_noise} is a fixed version of the fixation points from the app (See Eye-fixations Shiny App in OSF). In the discussion, implications of removal standards based on eye-fixation alone are considered and discussed as a signal detection problem.

As displayed in figure \ref{fig:signal_noise}, fixations are mostly distributed at the center of the screen, indicating no looks to quadrants. Whereas this remains true for competitor items throughout the trial, target items begin to move toward visual stimuli as early as the verb onset and much more extreme in later time frames. Crucially, however, the fixations do not always reach the actual quadrants. In analyzing the data from the Shiny app, removing data between the center point of the screen and the inner-edges of the quadrants results in \~83.33\% data loss, which is more than twice as high as previous reports on two image stimuli web-based studies \parencite{Vos_2017}. However, if we move to a more relaxed categorization of simply splitting the screen into four quadrants, then only 6.71\% of data is lost. In contrast, maximal outer-edge removal results in very little data loss (max \~32\%). When removing inner-edge eye-fixations, the choice comes down to removing signal to avoid noise in spatial ambiguity, or embrace noise to maximally retain the signal. As shown in the competitors-time 800 (upper-right) section of figure \ref{fig:signal_noise}, the noise is randomly distributed across quadrants just as it is early in the trial before eye-movements tend toward visual stimuli. Here, we aim to strike the balance of the signal-noise trade off by removing most of the data outside the screen size and by maximally retaining inner data that shows trends. This leads us to believe that no bias would occur even if classifying data from the x, y fixation center (0.5, 0.5). 


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/signal_noise.pdf}
    \caption{Quadrant locations and actual screen sizes are denoted with white lines}
    \label{fig:signal_noise}
\end{figure}

From L: 180-190, we create a classification system based on no inner-edge removal of the eye-fixations and partial removal of outer-edge eye-fixations (the code was created with inner removal in mind so that future researchers can simply adapt the distance variable L: 177, if desired). We use two types of control flow to first classify eye-fixations into quadrants and then create binary variables to link the quadrant to the visual stimuli. 
\inlineR{case\_when()} is used (L: 180-190) because of the multiple conditions and because \inlineR{case\_when()} is only truth evaluating, meaning that it only provides a specific output in the case of something being true. For example, if we only want to classify images that are within a particular space and leave others blank, then non-binary classification like \inlineR{case\_when()} is optimal. In contrast, if the outcomes of a classification are binary, then \inlineR{ifelse()} is an effective solution. For example, L: 192-200 makes a binary decision on whether an image being viewed is the same or different from the target (L: 193), competitors (L: 194-195), and distractor (L: 196), separately. While complexity of implementation may vary, logically, either can be used to achieve the same result in all cases with the use of operators and/or nesting.

\lstinputlisting[style=mystyle, firstnumber=171]{scripts/chunk-ET Data: Localizing Visual Stimuli.R}

In addition to more variable eye-fixation, web-based eye-tracking also has variable frame rates. That is, the frame rates across trials and between participants at any given point are variable \parencite[][]{Vos_2017}{}{}. Figure \ref{fig:Participant_frame_rates} shows a categorization of participants by median frame rate across trials. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/Participant_frame_rates.pdf}
    \caption{Mean frame rate is marked with dotted horizontal line}
    \label{fig:Participant_frame_rates}
\end{figure}
Like other recent web-based eye-tracking studies, our mean frame rate was ~20Hz (m:~22.17Hz, SD:11.61). Here, we remove the five participants with less than 5Hz median frame rates and create time bins by first creating a standard for removal in L: 378 and a binning size (L: 379). We then aggregate by participant \inlineR{Participant.Private.ID}, item \inlineR{subject\_img\_file}, and condition \inlineR{verb\_type} \inlineR{talker} (L: 381) in order to remove all participants that are below the predetermined median (L: 381-388). Next, time bins are created by normalizing the time range for each item (L: 389). Additionally, we subtracted 200 ms for human eye movements to occur (this step was not explicit in \textcite{Porretta_et_al_2020}, but we recommend future researchers always make this step explicit). Said another way, we center the time so that 0 is always the onset of the verb of interest. After normalizing, bins are created by dividing the time \inlineR{time\_elapsed} by the bin size \inlineR{time\_binning}, rounding, then multiplying by the bin size \inlineR{time\_binning} (L: 390), which is simply rounding items to the nearest bin size number.

\lstinputlisting[style=mystyle, firstnumber=377]{scripts/chunk-All Data: Clean and Tidy.R}

Creating time bins is fundamentally discretizing a continuous scale. In any fixed set of eye-tracking data, the grain size of the time scale has an inverse relationship to the amount of data in each time bin. Said another way, if you increase the bin size, you will have more data per bin, but less bins across time. Many statistical analyses can bypass the binning procedure altogether by keeping time a continuous variable. Nevertheless, for analyses that do require time bins and for visualization alone, it is worth exploring whether specific bin sizes affect a researcher's ability to capture an effect. To do this, we created a second Shiny app that is depicted in figure \ref{fig:frame_rates_over_trial} (see Frame Rate Shiny App in OSF), which allows the reader to explore the interactions between data removal based on participant median frame rates, changing bin sizes, and seeing output in the form of empirical logits for either linear lines or GAMM smoothed curves. Here, two crucial discoveries are made. First, time bin sizes have almost no effect on an \inlineR{verb_type} being captured. That is, almost any arbitrary sized bin can allow for capturing the effect of \inlineR{verb_type}, with the caveat of the bin needing to be several sizes smaller than the window of interest. Second, nearly any frame rate of data can capture the effect outside very small frame rates of 5Hz and below. If only examining data that is 6-11Hz, the effect of \inlineR{verb\_type} for \inlineR{talker} start to become apparent. And, for the Chinese-accented speaker the \inlineR{verb\_type} becomes apparent between 12-17Hz. 


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/frame_rates_over_trial.pdf}
    \caption{Total looks per bin size. }
    \label{fig:frame_rates_over_trial}
\end{figure}

The final step before visualization and statistical analysis is a final tidying. Like the first wrangling that we did, we create a tidy data frame through removal. Here, all eye-fixations that are outside the window of interest (-400ms and 800ms) are removed. Now, our new tidy data is structured based on the core four. For each participant, each audio stimuli and visual stimuli set is classified by \inlineR{talker} and \inlineR{verb\_type}. Finally, we have removed all times outside the window of interest. By tidying in this way, eye-fixations become meaningful in that each row is classified into looks to targets, competitors, and distractors. That is, each row is a classified eye-fixations based on a specific time, for each participant, and for varying conditions.

Between the two data frames \inlineR{all\_data\_cleaned} and \inlineR{all\_data\_tidy}, we have all of the behavioral data ready for any analysis or exploration that can be done. 

    


