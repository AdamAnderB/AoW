---
title: "R_workflow"
output: 
  pdf_document:
    keep_tex: true

---
```{r libraries, message=FALSE, error=FALSE}
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(readxl)
library(lmerTest)
library(lme4)
library(mgcv)
```
```{r data_load,echo=FALSE}
#set the path to be the folder where the et_data.csv,OSF_data.csv, 
#and task_data.csv are located
#path<-"/Users/adambramlett/scripts/the_art_of_wrangling/"
#input_path<-file.path(path,"data")
#output_path<-file.path(path,"cleaned_data")
#shiny_path<-file.path(path,"shiny_apps")
#viz_path<-file.path(path,"AoW/figures")
```
Here, three dialogue boxes will pop up. You simply need to select task data from the file you downloaded from the OSF link. The rest should run automatically


```{r Data Reading}
#select task_data
task_data_select <- file.choose()
task_data<-read.csv(task_data_select,header=TRUE, row.names=1)
#change for ET data
et_data_select<-sub("task_data", "et_data", task_data_select)
eyetracking_data<-read.csv(et_data_select,header=TRUE, row.names=1)
#change for OSF data
OSF_data_select<-sub("task_data", "OSF_data", task_data_select)
OSF_data<-read.csv(OSF_data_select,header=TRUE, row.names=1)
```
```{r Questionnaire: Clean}
cleaned_quest_data<-task_data%>%
  filter(display=="questionairre",na.omit=TRUE)%>%
  select(Participant.Private.ID,Screen.Name,Response,Reaction.Time)%>%
  filter(Response != "",Reaction.Time!=0)%>%
  select(!Reaction.Time)
```
```{r Questionnaire: Tidy}
tidy_quest_data<-cleaned_quest_data%>% 
  group_by(Participant.Private.ID,Screen.Name)%>%
  summarise_all(toString)%>%
  pivot_wider(names_from=Screen.Name,values_from=Response)%>%
  mutate(speaks_L2 =if_else(str_detect(other_languages_spoken,"German") &
                     !is.na(other_languages_spoken),1,0),
         across(c(chinese_study_duration,age,experience_chinese_accent),
                as.numeric),
         Participant.Private.ID = as.factor(Participant.Private.ID))%>%
  select(!other_languages_spoken)
```
```{r Questionnaire: visualization numeric variables}
tidy_quest_data%>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```
```{r Questionnaire: Filtered}
filtered_quest_data<-tidy_quest_data%>%
  filter(age<=40 & age>=18, #1 removed for age range
         chinese_study_duration==0,  #none removed 
         speaks_L2==0,#2 removed that speak other languages
         language_disorder == "No") #none removed
```
```{r Experimental Data: Clean and Tidy}
experimental_cleaned <- task_data%>%
  filter(Participant.Private.ID %in% 
           filtered_quest_data$Participant.Private.ID)%>%
  filter(Zone.Type == "response_button_image"|
           Zone.Type == "response_button_text")%>%
  filter(verb_type == "Restricting" |verb_type == "NonRestricting")%>%
  select_if(~sum(!is.na(.)) > 0)

experimental_tidy<-experimental_cleaned%>%
  select(!c(Event.Index:Local.Date, 
            Screen.Number:Zone.Name,
            Reaction.Time:Response.Type))%>%
  pivot_wider(names_from = Zone.Type,values_from = Response)%>%
  mutate(subject_img_file=center_image) #for renamed matching in next step
```
```{r OSF Data: Clean and Tidy}
OSF_filt <- OSF_data%>%
  select(talker,verb_type,subject_img_file,img_1_file, img_2_file, 
         img_3_file, img_4_file,log_SUBTLWF_Obj)
```
```{r Behavioral Data: Join OSF and Experimental Data}
behavioral_data <-experimental_tidy%>%
  left_join(OSF_filt, by=c( "talker","verb_type","subject_img_file"))
```
```{r Behavioral Data: Clean and Tidy}
behavioral_data <-behavioral_data %>%
  mutate(participant = as.factor(Participant.Private.ID),
         image_incorrect= if_else(img_1_file == response_button_image,0,1),
         text_incorrect = if_else(response_button_text == "Yes",0,1))
```
```{r Behavioral Data: Removal Standards}
#These are in standard deviations to retain maximum amount of quality data
#We set all of these to be 3 SDs, code here is only for your future use
image_participant_threshold = 3
image_item_threshold = 3
text_participant_threshold = 3
text_item_threshold = 3
```
```{r Behavioral Data: Participant and Item Removal}
#participant removal
participant_agg<-behavioral_data%>%
  group_by(Participant.Private.ID)%>%
  summarize(num_incorrect_image=sum(image_incorrect),
            num_incorrect_text=sum(text_incorrect))%>%
  mutate(mean_image_score = mean(num_incorrect_image),
         sd_image_score = sd(num_incorrect_image),
         mean_text_score = mean(num_incorrect_text),
         sd_text_score = sd(num_incorrect_text))%>%
  filter(num_incorrect_image <= mean_image_score+
           (sd_image_score*image_participant_threshold) &
         num_incorrect_text <= mean_text_score+
           (sd_text_score*text_participant_threshold))
#item removal
item_agg<-behavioral_data%>%
  group_by(center_image)%>%
  summarize(num_incorrect_image=sum(image_incorrect),
            num_incorrect_text=sum(text_incorrect))%>%
  mutate(mean_image_score = mean(num_incorrect_image),
         sd_image_score = sd(num_incorrect_image),
         mean_text_score = mean(num_incorrect_text),
         sd_text_score = sd(num_incorrect_text))%>%
  filter(num_incorrect_image <= mean_image_score+
           (sd_image_score*image_item_threshold) &
         num_incorrect_text <= mean_text_score+
           (sd_text_score*text_item_threshold))

behavioral_data <-behavioral_data%>%
  filter(image_incorrect == 0 & text_incorrect == 0)
```
```{r Behavioral Data: Removing with IN Operator}
behavioral_data<-behavioral_data%>%
  filter(Participant.Private.ID %in% participant_agg$Participant.Private.ID &
           center_image %in% item_agg$center_image)%>%
  select(-c(text_incorrect,image_incorrect,response_button_text))
```
```{r ET Data: Tidying and Filtering with an Inner Join}
et_data<-eyetracking_data%>%
  select(time_elapsed,participant_id,spreadsheet_row,
         type,x_pred_normalised,y_pred_normalised)%>%
  filter(type =="prediction" )%>%
  rename("Participant.Private.ID"="participant_id",
         "Spreadsheet.Row"="spreadsheet_row")
```

```{r,eval = FALSE,echo=FALSE,}
#this chunk of code was created for the purposes of make the shiny app in 
#a seperate script. Can be ignored if you are only interested in the analysis
shinny_binning_data<-behavioral_data %>% 
  left_join(et_data, by=c("Participant.Private.ID", "Spreadsheet.Row"))
shinny_binning_data <- apply(shinny_binning_data,2,as.character)
write.csv(shinny_binning_data,file.path(shiny_path,"Frame_Rate_App/shiny_binning_data.csv"))
write.csv(shinny_binning_data,file.path(shiny_path,"Eye_Fixations_App/shiny_binning_data.csv"))
```
```{r ET Data: Localizing Visual Stimuli}
#logically equivalent to doing full join and removing non-experimental trials.
all_data <-behavioral_data %>% 
  left_join(et_data, by=c("Participant.Private.ID", "Spreadsheet.Row"))

center=.5#center of screen
distance=0#distance to visual stimuli
beyond_screen=1 #distance to beyond_screen

all_data<-all_data%>%
  mutate(image_viewing=
  case_when(x_pred_normalised <= center-distance & 
              y_pred_normalised >= center+distance ~ image_1,
            x_pred_normalised >= center+distance & 
              y_pred_normalised >= center+distance ~ image_2,
            x_pred_normalised <= center-distance & 
              y_pred_normalised <= center-distance ~ image_3,
            x_pred_normalised >= center+distance & 
              y_pred_normalised <= center-distance ~ image_4))%>%
  filter(!is.na(image_viewing))

all_data<-all_data %>% 
  mutate(target = if_else(image_viewing == img_1_file, 1, 0), 
         comp_1 = if_else(image_viewing == img_2_file, 1, 0), 
         comp_2 = if_else(image_viewing == img_3_file, 1, 0), 
         dist = if_else(image_viewing == img_4_file, 1, 0))%>%
  filter(x_pred_normalised>center-beyond_screen &
           x_pred_normalised<center+beyond_screen&
         y_pred_normalised>center-beyond_screen &
           y_pred_normalised<center+beyond_screen)
```
```{r}
time_start = -500
time_end = 900
time_bin_size<-400
actual_distance=.3
actual_screen=.5

viz<-all_data%>%
  mutate(time_elapsed=time_elapsed-object_start-200)%>%
  mutate(time_elapsed_rounded=time_bin_size*round((time_elapsed)/time_bin_size))%>%
  filter(time_elapsed_rounded>time_start &time_elapsed_rounded<time_end)%>%
  mutate(object_viewing = if_else(target == 1,"Target",
                                  if_else(comp_1 == 1,"Competitors",
                                          if_else(comp_2 == 1,"Competitors",
                                                  if_else(dist == 1,"Competitors","")))))
viz<-viz%>%
  mutate(time_elapsed_round_coded = 
           case_when(as.character(time_elapsed_rounded) == "-400"~"-400 \nSentence Onset",
                     as.character(time_elapsed_rounded) == "0"~"0 \nVerb Onset",
                     as.character(time_elapsed_rounded) == "400"~ "400 \nObject Onset",
                     as.character(time_elapsed_rounded) == "800"~"800 \nSelection of Visual Stimuli"))
viz%>%
  ggplot(aes(x=x_pred_normalised,y=y_pred_normalised,color = as.factor(object_viewing)))+
  stat_density_2d(geom = "raster",
  aes(fill = after_stat(density)),
  contour = FALSE)+
  #geom_point(alpha=.1,size=3)+
  #scale_fill_viridis_c(option="inferno")+
  scale_fill_gradient(low = "white", high = "#C41230")+
  annotate("rect", 
           xmin = center-actual_screen, xmax = center+actual_screen, 
           ymin = center-actual_screen, ymax = center+actual_screen
           ,alpha = 0, color= "white")+
  annotate("rect", 
           xmin = center-actual_screen, xmax = center-actual_distance, 
           ymin = center+actual_distance, ymax = center+actual_screen,
             alpha = 0, color= "white")+
  annotate("rect", 
           xmin = center+actual_distance, xmax = center+actual_screen, 
           ymin = center-actual_screen, ymax = center-actual_distance,
             alpha = 0, color= "white")+
  annotate("rect", 
           xmin = center-actual_screen, xmax = center-actual_distance, 
           ymin = center-actual_screen, ymax = center-actual_distance,
             alpha = 0, color= "white")+
  annotate("rect", 
           xmin = center+actual_screen, xmax = center+actual_distance, 
           ymin = center+actual_distance, ymax = center+actual_screen,
             alpha = 0, color= "white")+
  facet_grid(as.factor(object_viewing)~time_elapsed_round_coded)+
  theme_minimal()+
  theme(legend.position="none")+
  ylab("Screen height - Y axis")+
  xlab("Screen width - X axis")+
  scale_y_continuous(breaks =c(0,.5,1))+
  scale_x_continuous(breaks =c(0,.5,1))+
  theme(panel.spacing = unit(0, "cm"),
        axis.ticks.x=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.text.x = element_text(face="bold", color="#C41230", 
                           size=14, angle=0),
        axis.text.y = element_text(face="bold", color="#C41230", 
                           size=14, angle=0))
ggsave(file.path(viz_path,"signal_noise.pdf"), width = 8, height = 4)
```
```{r ET Data: Varibility and Frame Rates}
filter_median_hz_min<-0
filter_median_hz_max<-1000

participant_frame_rate<-all_data%>%
  group_by(Participant.Private.ID,subject_img_file,verb_type,talker)%>%
  summarize(count = n(),
            max_time = max(time_elapsed),
            frame_rate = count/max_time*1000)%>%
  ungroup()%>%
  group_by(Participant.Private.ID)%>%
  mutate(median_frame_rate = median(frame_rate),
         frame_rate_cats = case_when(median_frame_rate>=30~"high",
                                     median_frame_rate<30&median_frame_rate>=5~"medium",
                                     median_frame_rate<5~"low"),
         frame_rate_cats=as.factor(frame_rate_cats))%>%
  ungroup()%>%
  group_by(Participant.Private.ID)%>%
  filter(median_frame_rate>filter_median_hz_min & median_frame_rate<filter_median_hz_max)
overall_mean_rate<- mean(participant_frame_rate$frame_rate)
sd(participant_frame_rate$frame_rate)
library(ggExtra)
plotter<-participant_frame_rate%>%
  ggplot(aes(y=frame_rate,
                   x=fct_reorder(as.factor(Participant.Private.ID), median_frame_rate),
                   group=as.factor(Participant.Private.ID),color =frame_rate_cats))+
  geom_jitter(alpha=.3)+
  geom_violin()+
  geom_boxplot(alpha=.2)+
  geom_point(aes(y=median_frame_rate),color ="white")+
  geom_hline(yintercept = overall_mean_rate,linetype=3)+
  theme_minimal()+
  scale_color_manual(values = c("#009647","#FDB515","#EF3A47"))+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.major.x = element_blank(),
        legend.position = c(.1,.75),
        legend.direction="vertical",
        legend.title = element_blank())+
  xlab("Participants Ordered by Median Frame Rate")+
  ylab("Frame Rate (Hz)")

ggMarginal(plotter, groupColour = TRUE, groupFill = TRUE)
ggsave(file.path(viz_path,"Participant_frame_rates.pdf"), width = 8, height = 4)

#frame rates across time
time_binning=400

binned_data<-all_data%>%
  mutate(time_elapsed=time_elapsed-object_start-200)%>% 
  mutate(time_elapsed_rounded=time_binning*round((time_elapsed)/time_binning))%>%
  filter(time_elapsed<2400&time_elapsed>-2400)

frame_rate_cats<-participant_frame_rate%>%
  select(frame_rate_cats,Participant.Private.ID,subject_img_file)%>%
  distinct(Participant.Private.ID,frame_rate_cats)

#across participants by item
agg_data_item <- binned_data%>% 
  group_by(time_elapsed_rounded,subject_img_file) %>%
  summarise(target_n=n()) 

#participant items by participants
agg_data_part <- binned_data%>% 
  group_by(time_elapsed_rounded,Participant.Private.ID)%>%
  summarise(target_n=n())%>%
  full_join(frame_rate_cats)

ggplot()+
  geom_vline(xintercept = -400,color="#007BC0")+
  geom_vline(xintercept = 800,color ="#007BC0")+
  geom_jitter(data=agg_data_part,aes(x=time_elapsed_rounded,
                                     y=target_n,
                                     color = frame_rate_cats),alpha=.2)+
  geom_boxplot(data=agg_data_part,aes(x=time_elapsed_rounded,
                                      y=target_n,
                                      alpha=.1,
                                      group =interaction(time_elapsed_rounded,frame_rate_cats),
                                      color=frame_rate_cats,
                                      fill=frame_rate_cats),
               alpha=.4)+
  geom_jitter(data=agg_data_item,aes(x=time_elapsed_rounded,
                                     y=target_n),
              alpha=.02,
              color = "grey")+
  geom_violin(data=agg_data_item,aes(x=time_elapsed_rounded,
                                     y=target_n,
                                     alpha=.1,
                                     group=time_elapsed_rounded),
              color = "grey",
              fill= "grey",alpha=.1)+
  scale_fill_manual(values = c("#009647","#FDB515","#EF3A47"))+
  scale_color_manual(values = c("#009647","#FDB515","#EF3A47"))+
  scale_x_continuous(limits = c(-2200,2400),breaks=seq(-2400,2400, 400))+
  theme_minimal()+
  theme(axis.ticks.x=element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = c(.8,.75),
        legend.direction="vertical",
        legend.title = element_blank())+
  xlab("Adjusted Time")+
  ylab("Total Looks Captured")
ggsave(file.path(viz_path,"frame_rates_over_trial.pdf"), width = 8, height = 4)
```
```{r All Data: Clean and Tidy}
frame_rate_cut_off<-5
time_binning<-50
all_data_cleaned<-all_data%>%
  group_by(Participant.Private.ID,subject_img_file,verb_type,talker)%>%
  mutate(count = n(),
            max_time = max(time_elapsed),
            frame_rate = count/max_time*1000)%>%
  ungroup()%>%
  group_by(Participant.Private.ID)%>%
  mutate(median_frame_rate = median(frame_rate))%>%
    filter(median_frame_rate>=frame_rate_cut_off)%>%
  mutate(time_elapsed=time_elapsed-object_start-200)%>% 
  mutate(time_elapsed_rounded=time_binning*round((time_elapsed)/time_binning))

all_data_tidy <- all_data_cleaned%>%
  filter(time_elapsed_rounded>=-400 & time_elapsed_rounded<=800)
```
```{r All Data: Preparing for Visualization}
all_data_tidy_viz <- all_data_tidy%>% 
  group_by(time_elapsed_rounded,verb_type, talker,Participant.Private.ID)%>%
  summarise(target_looks = mean(target), 
            comp_1_looks = mean(comp_1),
            comp_2_looks = mean(comp_2),
            dist_looks = mean(dist),
            target_n=n())%>%
  mutate(emp_logit =log((target_looks+(0.5/target_n))/
                          (1-(target_looks+(0.5/target_n)))))
```
```{r Emp Logits visualizations}
viz<-ggplot(all_data_tidy_viz, 
                  aes(x=time_elapsed_rounded, y=emp_logit, 
                      linetype = verb_type, fill = talker,color= talker))
point_viz<-viz+geom_line()
line_viz<-viz+geom_smooth(method = "lm")
smooth_viz<-viz+geom_smooth()

point_viz+scale_x_continuous(limits = c(-400,800),breaks=seq(-400,800, 400))+
  scale_linetype_discrete(labels=c('Non-Restricting','Restricting'))+
  scale_color_manual(values=c("#007BC0", "#FDB515"),labels=c('Native','Non-Native'))+
  ylab("Average empirical logit looks to object")+
  xlab("Time (ms)")+
  theme(legend.position = c(.2, .7))+
  theme_minimal()+
  labs(title = "Online Replication (upper) vs. in person data (lower):",
              subtitle = "Lower visual is from Poretta et al's (2020)")+
  labs(color='Talker',
       linetype="Verb Type")+
  theme(legend.position = c(.2,.7),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.key=element_blank())+
  guides(color = guide_legend(override.aes = list(fill = NA)),
         linetype = guide_legend(override.aes = list(fill = NA)))+facet_wrap(Participant.Private.ID~1)

line_viz+scale_x_continuous(limits = c(-400,800),breaks=seq(-400,800, 400))+
  scale_linetype_discrete(labels=c('Non-Restricting','Restricting'))+
  scale_color_manual(values=c("#007BC0", "#FDB515"),labels=c('Native','Non-Native'))+
  scale_fill_manual(values=c("#007BC0", "#FDB515"),labels=c('Native','Non-Native'))+
  ylab("Average empirical logit looks to object")+
  xlab("Time (ms)")+
  theme(legend.position = c(.2, .7))+
  theme_minimal()+
  labs(title = "Online Replication (upper) vs. in person data (lower):",
              subtitle = "Lower visual is from Poretta et al's (2020)")+
  labs(color='Talker',
       linetype="Verb Type")+
  theme(legend.position = c(.2,.7),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.key=element_blank())+
  guides(color = guide_legend(override.aes = list(fill = NA)),
         linetype = guide_legend(override.aes = list(fill = NA)))+facet_wrap(Participant.Private.ID~1)

smooth_viz+scale_x_continuous(limits = c(-400,800),breaks=seq(-400,800, 400))+
  scale_linetype_discrete(labels=c('Non-Restricting','Restricting'))+
  scale_color_manual(values=c("#007BC0", "#FDB515"),labels=c('Native','Non-Native'),guide="none")+
  scale_fill_manual(values=c("#007BC0", "#FDB515"),labels=c('Native','Non-Native'))+
  ylab("Average empirical logit looks to object")+
  xlab("Time (ms)")+
  theme(legend.position = c(.2, .7))+
  theme_minimal()+
  labs(title = "",
              subtitle = "Our Web-Based Replication")+
  labs(color='Talker',
       linetype="Verb Type")+
  theme(legend.position = c(.2,.7),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.key=element_blank())+
  guides(color = guide_legend(override.aes = list(fill = NA)),
         linetype = guide_legend(override.aes = list(fill = NA)),
         fill = FALSE)+facet_wrap(Participant.Private.ID~1)
ggsave(file.path(viz_path,"smooth_plot_individual_differences.pdf"), width = 8, height = 20)
```
```{r All Data: Preparing for Models}
mem_data<-all_data_tidy%>%
  select(Participant.Private.ID,verb_type,talker,
         subject_img_file,target,Trial.Number,log_SUBTLWF_Obj,target_obj,time_elapsed)%>%
  mutate(Participant.Private.ID=as.factor(Participant.Private.ID))%>%
  left_join(tidy_quest_data)

gamm_data<-mem_data%>%
  mutate(Condition = paste(talker,verb_type,sep="."))
```

```{r GLMER: Leveling the Data}
mem_data$verb_type<-as.factor(mem_data$verb_type)
mem_data$talker<-as.factor(mem_data$talker)
contrasts(mem_data$verb_type)
contrasts(mem_data$talker)
contrasts(mem_data$verb_type)<-c(-.5,.5)
contrasts(mem_data$talker)<-c(-.5,.5)
colnames(contrasts(mem_data$talker))<- c('native')
colnames(contrasts(mem_data$verb_type))<- c('nonrestricting')
```
```{r GLMER: Model Comparison}
#time correction
# maximal model:(failed to converge)---
m2<-glmer(target~talker*verb_type+
            (talker|subject_img_file)+
            (verb_type|Participant.Private.ID),
          family="binomial",data=mem_data)
summary(m2)

# Near maximal mode:l (failed to converge)---
m3<-glmer(target~talker*verb_type+
            (talker||subject_img_file)+
            (verb_type||Participant.Private.ID),
          family="binomial",data=mem_data)
summary(m3)

#by item random effects removed first for first iteration
# parsimonious model (removing item random effects but keeps random slopes for verb type---singularity 
m4<-glmer(target~talker*verb_type+
            (1|subject_img_file)+
            (verb_type||Participant.Private.ID),
          family="binomial",data=mem_data)
summary(m4)

#by item and subject random effects removed for second iteration
# parsimonious model 2
m5<-glmer(target~talker*verb_type+
            (1|subject_img_file)+
            (1|Participant.Private.ID),
          family="binomial",data=mem_data)
summary(m5)
anova(m4,m5)
```
```{r GAMM: Leveling the Data}
# turn categorical variables into factors with numeric values 
gamm_data$subject_img_coded <- as.numeric(factor(gamm_data$subject_img_file)) -1
gamm_data$talker_coded <- as.numeric( factor(gamm_data$talker) ) -1 
gamm_data$verb_type_coded <- as.numeric( factor(gamm_data$verb_type) ) -1
gamm_data$Condition <- factor(gamm_data$Condition) 
gamm_data$event <- as.factor(paste(gamm_data$Trial.Number,gamm_data$talker_coded,sep="."))
gamm_data$Participant.Private.ID <- as.numeric( factor(gamm_data$Participant.Private.ID) ) -1
gamm_data$target_obj <- factor(gamm_data$target_obj) 

```
```{r GAMM: Model Comparison}
#base model
mod1 <- bam(target ~ Condition
                  + s(time_elapsed, by=Condition) +
                  + s(Trial.Number) + s(log_SUBTLWF_Obj) +
                  + s(time_elapsed, Participant.Private.ID, bs="fs", m=1)
                  + s(time_elapsed, target_obj, bs="fs", m=1)
                  + s(event, bs = "re")
            ,data=gamm_data,family="binomial",discrete=TRUE)
summary(mod1)


#without interaction
mod3 <- bam(target ~ experience
                  + s(time_elapsed, by=Condition) +
                  + s(Trial.Number) + s(log_SUBTLWF_Obj) +
                  + s(time_elapsed, Participant.Private.ID, bs="fs", m=1)
                  + s(time_elapsed, target_obj, bs="fs", m=1)
                  + s(event, bs = "re")
            ,data=gamm_data_accented,family="binomial",discrete=TRUE)
summary(mod3)

#with interaction
mod4 <- bam(target ~ experience
                  + te(time_elapsed, experience, by=verb_type_coded)
                  + s(time_elapsed, by=Condition) +
                  + s(Trial.Number) + s(log_SUBTLWF_Obj) +
                  + s(time_elapsed, Participant.Private.ID, bs="fs", m=1)
                  + s(time_elapsed, target_obj, bs="fs", m=1)
                  + s(event, bs = "re")
            ,data=gamm_data_accented,family="binomial",discrete=TRUE)
summary(mod4)
```

```{r}

eyetracking_dat<-gamm_data
eyetracking_dat <- transform(eyetracking_dat, 
                             Condition = ifelse(talker=="NativeMale" & verb_type == "NonRestricting",
                                                                 "Native.NonRestricting",
                                                ifelse(talker=="NativeMale" & verb_type == "Restricting",
                                                                       "Native.Restricting",
                                                       ifelse(talker=="NonNativeMale" & verb_type ==
                                                                "NonRestricting","NonNative.NonRestricting", "NonNative.Restricting"))))
eyetracking_dat$Condition <- as.factor(eyetracking_dat$Condition)




# UNUSED: turn categorical variables into factors with numeric values 
eyetracking_dat$subject_img_coded <- as.numeric( factor(eyetracking_dat$subject_img_file) ) -1
# levels(factor(eyetracking_dat$subject_img_file)) starts from 0
eyetracking_dat$talker_coded <- as.numeric( factor(eyetracking_dat$talker) ) -1 
# NativeMale = 0, NonNativeMale = 1
eyetracking_dat$verb_type_coded <- as.numeric( factor(eyetracking_dat$verb_type) ) -1
# Nonrestricting = 0, Restricting = 1
eyetracking_dat$Participant.Private.ID <- as.factor(eyetracking_dat$Participant.Private.ID)

# Event is a factor in Poretta et al.'s model 
eyetracking_dat$Event <-as.factor(paste(eyetracking_dat$Participant.Private.ID,eyetracking_dat$Trial.Number,sep="."))
eyetracking_dat$Event <- as.factor(eyetracking_dat$Event)
eyetracking_dat$experience_chinese_accent
eyetracking_dat<-eyetracking_dat%>%
  select(Event,Participant.Private.ID,Trial.Number,verb_type_coded,talker_coded,subject_img_coded,Condition,target,time_elapsed,log_SUBTLWF_Obj,experience_chinese_accent,Event)
# orthogonal contrast coding: does not change significnat of model coefficients
# contrasts(eyetracking_dat$Condition) <- matrix(c(-0.25, 0.75, -0.25, -0.25,
#                                                  -0.25, -0.25, 0.75, -0.25,
#                                                  -0.25, -0.25, -0.25, 0.75), ncol=3)
#contrasts(eyetracking_dat$Condition)
```

# GAM 
```{r}
############## Poretta model #############################
# m1_trim <- bam(IA_1_ELogit ~ Condition + s(Time, by = Condition) + # main effect Condition (NR, NNR, NNR, NNNR); smooth for time by each factor of condition
#             s(Trial_c) + s(log_SUBTLWF_Obj) + # smooth for trial order; smooth for log frequency of target object 
#             s(Time, Subject, bs = "fs", m = 1) + # smooth random slope for each participant by Time  
#             s(Time, target_obj, bs = "fs", m = 1) + # smooth random slope for Target object and Time 
#             s(Event, bs = "re"),   # smooth random intercept for Event 
#           data = DatCor_400_0_800, weights = 1/IA_1_wts,  # weight
#           AR.start = StartEvent, method = "ML", rho = 0.740431,  # method of Maximum Likelihood to estimate lambda the smoothing parameter (the penalty), Baysian approach rather than predictive mode 
#          subset = abs(scale(resid(m1))) < 2.5)
# no k set -> no expectation for the maximum wiggliness of the functions 
########################################################
any(is.na(eyetracking_dat))
View(eyetracking_dat)

library(bam)
mod1 <- bam(target ~ talker_coded + 
              s(time_elapsed, by=talker_coded) + 
              verb_type_coded + 
              s(time_elapsed, by=verb_type_coded) +
              talker_coded:verb_type_coded + 
              s(time_elapsed, by=Condition)+
              s(log_SUBTLWF_Obj)+
              s(time_elapsed, Participant.Private.ID, bs="fs", m=1)+
              s(time_elapsed, subject_img_coded, bs="fs", m=1)+
              s(Event, bs="re"), 
                  family="binomial", data=eyetracking_dat, discrete=TRUE, method="fREML")
summary(mod1)
library(sjPlot)
plot_model(mod1, type = "pred", terms = c("talker", "time_elapsed_rounded"))

plot_model(mod1, type = "std", title = "Model Plot")
plot_model(mod1, type = "int", terms = c("talker", "verb_type"))
plot_model(mod1, type = "pr")

mod1


#without interaction
exp_gam_mod1 <- bam(target ~ experience_chinese_accent + 
              s(time_elapsed, by=talker_coded) + 
              verb_type_coded + 
              s(time_elapsed, by=verb_type_coded) +
              talker_coded:verb_type_coded + 
              s(time_elapsed, by=Condition)+
              s(log_SUBTLWF_Obj)+
              s(time_elapsed, Participant.Private.ID, bs="fs", m=1)+
              s(time_elapsed, subject_img_coded, bs="fs", m=1)+
              s(Event, bs="re"), 
                  family="binomial", data=eyetracking_dat, discrete=TRUE, method="fREML")
summary(exp_gam_mod1)

#with interaction
exp_gam_mod2 <- bam(target ~ experience
                  + te(time_elapsed, experience, by=verb_type_coded)
                  + s(time_elapsed, by=Condition) +
                  + s(Trial.Number) + s(log_SUBTLWF_Obj) +
                  + s(time_elapsed, Participant.Private.ID, bs="fs", m=1)
                  + s(time_elapsed, target_obj, bs="fs", m=1)
                  + s(event, bs = "re")
            ,data=gamm_data_accented,family="binomial",discrete=TRUE)
summary(mod4)

```
sj