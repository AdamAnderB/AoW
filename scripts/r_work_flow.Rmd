---
title: "R_workflow"
output: 
  pdf_document:
    keep_tex: true
---
```{r libraries, message=FALSE, error=FALSE}
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(readxl)
```

```{r data_load,echo=FALSE}
#set the path to be the folder where the et_data.csv,OSF_data.csv, and task_data.csv are located
path<-"/Users/adambramlett/scripts/the_art_of_wrangling/"
input_path<-file.path(path,"data")
output_path<-file.path(path,"cleaned_data")

#change screens to question in task
task_data<-read.csv(file.path(input_path,"task_data.csv"))
eyetracking_data<-read.csv(file.path(input_path,"et_data.csv"))
OSF_data<-read.csv(file.path(input_path,"OSF_data.csv"))
```

```{r Questionnaire: Clean}
###participant removal by questionnaire
cleaned_quest_data<-task_data%>%
  filter(display=="questionairre",na.omit=TRUE)%>%
  select(Participant.Private.ID,Screen.Name,Response,Reaction.Time)%>%
  filter(Response != "",Reaction.Time!=0)%>%
  select(!Reaction.Time)
```

```{r Questionnaire: Tidy}
tidy_quest_data<-cleaned_quest_data%>%
  pivot_wider(names_from = Screen.Name, values_from = Response,values_fn = list)%>%
  rowwise()%>%
  mutate(across(c(accent_heard,other_languages_spoken), toString),
         across(where(is.character), tolower),
         across(c(chinese_study_duration,age,experience_chinese_accent), as.numeric),
         across(c(Participant.Private.ID), as.factor),
         speaks_L2 = if_else(other_languages_spoken == "english"|
                                         other_languages_spoken == "none"|
                                         other_languages_spoken == ""|
                                         other_languages_spoken == " ",0,1))
```
```{r Questionnaire: visualization numeric variables}
#visualize numeric variables
tidy_quest_data%>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```
```{r Questionnaire: Filtered}
#filtered data
filtered_quest_data<-tidy_quest_data%>%
  filter(age<=40 & age>=18 &   #1 removed due to being outside of age range (part:7492145)
         chinese_study_duration==0 & #none removed 
         speaks_L2==0,#2 participants removed that speak other languages
         language_disorder == "No") #none removed

#sanity check
#length(unique(task_data$Participant.Private.ID))
#length(unique(filtered_quest_data$Participant.Private.ID))

```
```{r Experimental Data: Clean and Tidy}
#response for quadrant data
experimental_cleaned <- task_data%>%
  filter(Participant.Private.ID %in% filtered_quest_data$Participant.Private.ID)%>%
  select(talker,verb_type,center_image,
         image_1,image_2,image_3,image_4,
         Participant.Private.ID,Spreadsheet.Row,Trial.Number,
         Zone.Type,object_start,Response)%>%
  filter(Zone.Type == "response_button_image"|
           Zone.Type == "response_button_text" &  image_4 != "")%>%
  filter(verb_type == "Restricting" |verb_type == "NonRestricting")

experimental_tidy<-experimental_cleaned%>%
  pivot_wider(names_from = Zone.Type,values_from = Response)%>%
  mutate(subject_img_file=center_image)

```
```{r OSF Data: Clean and Tidy}

#combine OSF Data and experimental data data
#OSF for target,competitor, distractors
OSF_filt <- OSF_data%>%
  select(talker,verb_type,subject_img_file,type,
         img_1_file, img_2_file, img_3_file, img_4_file, 
         target_obj,pic_verification,object_start)%>% 
  filter(type == "Exp")


#sanity check
#length(unique(response_filt$Participant.Private.ID))
#length(unique(response_filt$center_image))
```
```{r Behaviorial Data: Clean and Tidy}
#participant and item removal (grouped) and individual trial removal
#combine data and filter out non experimental trials
behaviorial_data <- OSF_filt %>% 
  inner_join(experimental_tidy, by=c( "talker","verb_type","subject_img_file"))

#clean combined OSF and Response data
behaviorial_data$Participant.Private.ID<-as.factor(behaviorial_data$Participant.Private.ID)
behaviorial_data <-behaviorial_data %>%
  mutate(image_incorrect= if_else(img_1_file == response_button_image,0,1),
         text_incorrect = if_else(response_button_text == "Yes",0,1))
```
```{r Behaviorial Data: Removal Standards}
#These are in standard deviations to retain maximum amount of quality data
#We set all of these to be 3 SDs, but we put the code this way so you can change it for your needs
image_participant_threshold = 3
image_item_threshold = 3
text_participant_threshold = 3
text_item_threshold = 3

```
```{r Behaviorial Data: Participant and Item Removal}
#participant removal
participant_agg<-behaviorial_data%>%
  group_by(Participant.Private.ID)%>%
  summarize(num_incorrect_image=sum(image_incorrect),
            num_incorrect_text=sum(text_incorrect))%>%
  mutate(mean_image_score = mean(num_incorrect_image),
         sd_image_score = sd(num_incorrect_image),
         mean_text_score = mean(num_incorrect_text),
         sd_text_score = sd(num_incorrect_text))%>%
  filter(num_incorrect_image <= mean_image_score+(sd_image_score*image_participant_threshold) &
         num_incorrect_text <= mean_text_score+(sd_text_score*text_participant_threshold))
#item removal
item_agg<-behaviorial_data%>%
  group_by(center_image)%>%
  summarize(num_incorrect_image=sum(image_incorrect),
            num_incorrect_text=sum(text_incorrect))%>%
  mutate(mean_image_score = mean(num_incorrect_image),
         sd_image_score = sd(num_incorrect_image),
         mean_text_score = mean(num_incorrect_text),
         sd_text_score = sd(num_incorrect_text))%>%
  filter(num_incorrect_image <= mean_image_score+(sd_image_score*image_item_threshold) &
         num_incorrect_text <= mean_text_score+(sd_text_score*text_item_threshold))

individual_removal <-behaviorial_data%>%
  filter(image_incorrect == 0 & text_incorrect == 0)
```
```{r Behaviorial Data: Removal with %in%}
#removal of remaining incorrect individual items
behaviorial_data<-behaviorial_data%>%
  filter(Participant.Private.ID %in% participant_agg$Participant.Private.ID &
           center_image %in% item_agg$center_image) 
           #image_incorrect == 0 & text_incorrect == 0)

behaviorial_data <-behaviorial_data%>%
  select(-c(text_incorrect,image_incorrect,response_button_text,pic_verification,target_obj,type))
```
```{r ET Data: Clean-Tidy-Filtering with inner_join()}
#combine eye tracking with response and OSF data frames-- with data clean-up and manipulation
et_data<-eyetracking_data%>%
  select(time_elapsed,participant_id,spreadsheet_row,type,x_pred_normalised,y_pred_normalised)%>%
  filter(type =="prediction" )
et_data<-et_data%>%rename("Participant.Private.ID"="participant_id",
                          "Spreadsheet.Row"="spreadsheet_row")
et_data$Participant.Private.ID<-as.factor(et_data$Participant.Private.ID)
```
```{r ET Data: Localizing Visual Stimuli}
all_data <-behaviorial_data %>% inner_join(et_data, by=c("Participant.Private.ID", "Spreadsheet.Row"))

lower=.4
upper=.6

all_data<-all_data%>%
  mutate(image_viewing =  case_when(x_pred_normalised <= lower & y_pred_normalised >= upper ~ image_1,
                                  x_pred_normalised >= upper & y_pred_normalised >= upper ~ image_2,
                                  x_pred_normalised <= lower & y_pred_normalised <= lower ~ image_3,
                                  x_pred_normalised >= upper & y_pred_normalised <= lower ~ image_4))
all_data<-all_data%>%
  filter(image_viewing == image_1|
           image_viewing == image_2|
           image_viewing == image_3|
           image_viewing == image_4)

all_data<-all_data %>% 
  mutate(target = if_else(image_viewing == img_1_file, 1, 0), 
         comp_1 = if_else(image_viewing == img_2_file, 1, 0), 
         comp_2 = if_else(image_viewing == img_3_file, 1, 0), 
         dist = if_else(image_viewing == img_4_file, 1, 0))
```
```{r ET Data: Time-Empirical Logits/looks}
all_data<-all_data%>%
  mutate(time_elapsed=time_elapsed-object_start.y-200)%>%
  mutate(time_elapsed_rounded=50*round((time_elapsed)/50))

agg_data <- all_data %>% 
  group_by(time_elapsed_rounded,verb_type, talker)%>%
  summarise(target_looks = mean(target), 
            comp_1_looks = mean(comp_1),
            comp_2_looks = mean(comp_2),
            dist_looks = mean(dist),
            target_n=n()) 

agg_data<-agg_data%>%
  filter(time_elapsed_rounded>-400 & time_elapsed_rounded<800)

target_data<-agg_data%>%
  mutate(emp_logit =log((target_looks+(0.5/target_n))/(1-(target_looks+(0.5/target_n)))))


```
```{r}
all_data <- apply(all_data,2,as.character)
write.csv(target_data,file.path(output_path,"target_data.csv"))
write.csv(all_data,file.path(output_path,"all_data.csv"))
write.csv(agg_data,file.path(output_path,"agg_data.csv"))
```


all_data<-all_data%>%
  mutate(time_elapsed=time_elapsed-object_start.y-200)
all_data <- all_data %>% 
  mutate(time_elapsed_rounded=1000*round((time_elapsed)/1000))

all_data_f<-all_data%>%
  filter(time_elapsed_rounded>-400 & time_elapsed_rounded<800)

all_data_f<-all_data_f%>%
  filter(x_pred_normalised> -1 & x_pred_normalised<2&
         y_pred_normalised> -1 & y_pred_normalised<2)

all_data_f%>%
  ggplot(aes(x_pred_normalised,y_pred_normalised,color=time_elapsed_rounded))+
  geom_point()


all_data_f_agg<-all_data_f%>%
  group_by(time_elapsed_rounded,Participant.Private.ID,Trial.Number)%>%
  count()

all_data_f_agg%>%
  ggplot(aes(x=time_elapsed_rounded,y=n,group=as.factor(time_elapsed_rounded)))+
  geom_jitter()



``

#Taking a lookie at the viz

ggplot(target_data, aes(x=time_elapsed_rounded , y=emp_logit, linetype = verb_type, color= talker)) + 
  geom_line()+
  scale_color_manual(values=c("#003f7d", "#FD7702"))

ggplot(target_data, aes(x=time_elapsed_rounded , y=emp_logit, linetype = verb_type, color= talker)) + 
  geom_smooth(method = "lm")+
  scale_color_manual(values=c("#003f7d", "#FD7702"))

target_data%>%
  ggplot(aes(x=time_elapsed_rounded , y=target_looks, linetype =verb_type,color=talker))+geom_smooth()+
  scale_color_manual(values=c("#003f7d", "#FD7702"))+
  ylab("Average empirical logit looks to object")+
  xlab("Time (ms)")+
  theme(legend.position = c(.2, .7))+
  theme_minimal()+
  labs(title = "Online Replication (upper) vs. in person data (lower):",
              subtitle = "Lower visual is from Poretta et al's (2020)")

```


```

target_data$verb_type<-as.factor(target_data$verb_type)
contrasts(target_data$verb_type)
target_data$talker<-as.factor(target_data$talker)
contrasts(target_data$talker)

contrasts(target_data$verb_type)<-c(-.5,.5)
contrasts(target_data$talker)<-c(-.5,.5)

colnames(contrasts(target_data$talker))<- c('native')
colnames(contrasts(target_data$talker))<- c('nonrestricting')

m1<-lm(emp_logit~talker*verb_type,data=target_data)
summary(m1)
```

``
ana_data<-all_data

ana_data$verb_type<-as.factor(ana_data$verb_type)
contrasts(ana_data$verb_type)
ana_data$talker<-as.factor(ana_data$talker)
contrasts(ana_data$talker)

contrasts(ana_data$verb_type)<-c(-.5,.5)
contrasts(ana_data$talker)<-c(-.5,.5)

colnames(contrasts(ana_data$talker))<- c('native')
colnames(contrasts(ana_data$verb_type))<- c('nonrestricting')

#time correction
ana_data<-ana_data%>%filter(time_elapsed_rounded>-400 &time_elapsed_rounded<800)

# maximal model:(failed to converge)---
m2<-glmer(target~talker*verb_type+(talker|subject_img_file)+(verb_type|Participant.Private.ID),family="binomial",data=ana_data)
summary(m2)

# Near maximal mode:l (failed to converge)---
m3<-glmer(target~talker*verb_type+(talker||subject_img_file)+(verb_type||Participant.Private.ID),family="binomial",data=ana_data)
summary(m3)

#by item random effects removed first for first iteration
# parsimonious model (removing item random effects but keeps random slopes for verb type---singularity 
m4<-glmer(target~talker*verb_type+(1|subject_img_file)+(verb_type||Participant.Private.ID),family="binomial",data=ana_data)
summary(m4)

#by item and subject random effects removed for second iteration
# parsimonious model 2
m5<-glmer(target~talker*verb_type+(1|subject_img_file)+(1|Participant.Private.ID),family="binomial",data=ana_data)
summary(m5)

anova(m4,m5)
```



