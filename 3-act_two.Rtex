\section{Act 2: Replication of \textcite{Porretta_et_al_2020}}
\subsection{The replication}

We carried out a single change replication study \parencite{Marsden_2018} involving internet-based data collection of  \textcite{Porretta_et_al_2020}'s in-person VWP experiment, which found that a foreign accent impedes predictive processing but does not preclude it. \textcite{Porretta_et_al_2020} used a standard 2x2 VWP design discussed above \parencite[e.g.,][]{Allopenna_1998} to explore predictive sentence processing for accented and unaccented speech. On each trial (24 target, 24 filler), participants were presented with a 500-ms fixation cross followed by a 2x2 visual stimuli with an additional center image that represented the subject of the sentence. Each visual set of 2x2 visual stimuli were previewed for 200ms. Next, participants heard a transitive sentence that was either restrictive (e.g., “the fireman will climb the ladder”, \textit{climb} allows for object prediction) or unrestrictive (e.g., “the fireman will need the ladder”, \textit{need} does not allow for object prediction) and spoken by either a native or Chinese accented English speaker. Participants then must answer a simple comprehension question to ensure attention. All recordings were taken from \textcite{Porretta_et_al_2020} and speaker was counterbalanced across four lists. The experiment contains 250 images, 50 of which are center images and 200 that make up the 2x2 design (10 practice, 120 filler, 120 experimental). 99 of the images were identical to the original experiment (all 50 center images for subjects in the sentences and 49 of the visual stimuli for objects across practice, filler, and experimental items). The remaining 151 images were obtained following the same specifications of the initial study (open source line-drawn images). 

Whereas our study changed only the method of collecting data from in-person to online, the reader will see below, this single change leads to a variety of differences that required explicit choices to be made throughout data analysis. For example, inclusion and exclusion, classification of eye-fixations, time binning (adjusted time classification), frame rate (eye-fixation per time bin), unstable, among other issues. Below, we propose a number of best practices in this report to help improve transparency in these data wrangling choices, which we hope will become standard reporting procedures in both online and in-person eye-tracking studies, where applicable.

\subsection{"Enter: Wrangling"} 
\subsubsection{Questionnaire wrangling}
In this section, we will 

After combing data (see supplementary materials for methods on dealing with complex relational data bases), data wrangling starts with data removal. In a VWP experiment, removal occurs at four levels (questionnaire based, item based, behavior based, fixation quality based). While the exact point you start is unimportant, here, we start with simple first (questionnaire based removal). Our first data wrangling cycle question is: Which participants should be excluded based on not being qualified from information provided in the background questionnaire (e.g., not an English speaker and not between the ages of 18 and 40)? In line 43, we start with a clone of our behavioral data frame (113 columns and 881 rows) and assess needed variables (questionnaire questions, responses, participant ID, reaction time (RT)). RT is kept here because it allows for removing items that were unnecessarily generated from the experiment structure (i.e., getting rid of rows with 0 RT). All line numbers refer to actual line numbers in \inlineR{r\_work\_flow.rmd} and will be referred to with "L:+line number" (e.g., L: 156-157 refers to lines 156-157).

\lstinputlisting[language=R,firstnumber=42]{scripts/chunk-Questionnaire: Clean.R}

Now that we have a data frame with three columns (\inlineR{Participant.Private.ID}, \inlineR{Screen.Name}, \inlineR{Response}), we can restructure the data into tidy data (one observation per row and one variable per column) \parencite{Wickham_2014}. While this can be done with simple \inlineR{filter()} or \inlineR{select()} and joining, \inlineR{pivot\_wider()} and \inlineR{pivot\_longer()} offer a simple solution to this common data structure problem, see figure \ref{fig:pivoting}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/pivoting.png}
    \caption{Note: Pivoting Wider and Pivoting longer are both useful in creating tidy data in different situations. Experimental data (e.g., Gorilla-tasks, Psychopy, E-Prime) often requires widening, whereas questionnaire data (e.g., Gorilla-questionnaires, Google forms, Qualtrix) need to be pivoted longer.}
    \label{fig:pivoting}
    
\end{figure}

Starting at line 49, we pivot wider to create a single row for each participant with each question having its own column. One way to understand the importance of pivoting is by realizing that it is much easier to come up with standards for removal in the \inlineR{speaks\_L2}, \inlineR{age}, or \inlineR{hear\_impaired} columns than for the \inlineR{Response} column, which would require conditional standards based on \inlineR{Screen.Name}.

\lstinputlisting[language=R, firstnumber=49]{scripts/chunk-Questionnaire: Tidy.R}

Starting at line 69, we find that two participants should be removed for language expertise outside English and one for exceeding the age cut off (both predetermined values). We can filter this data to remove the participants that do not qualify. See supplementary materials for an example of visualizations created through specific data type (e.g., visualizing all numeric columns). Here we will stay focused on wrangling. 

\lstinputlisting[language=R,
firstnumber=69]{scripts/chunk-Questionnaire: Filtered.R}

We can now answer our question: Which participants should be excluded based on the background questionnaire? To finish this wrangling cycle, we can now use this data frame to filter out unqualified participants in the \inlineR{Participant.Private.ID} column of the next removal stage.


\subsubsection{Behavioral-task wrangling}

The next cycle of data wrangling begins with the question: Which participants and items should be removed based on the behavioral results of the experiment? Cleaning is similar to the questionnaire cycle, but we start from scratch with a clone of the behavioral data frame \inlineR{task\_behavior} because the new question has new goals, which requires different variables. However, we start this cycle's implementation by filtering the participants in the behavioral-task clone with the questionnaire data from above in order to only keep those participants that qualified in the questionnaire wrangling cycle (L: 77). We then remove all rows except ones relating to behavioral data questions (L: 78-79) and experimental items (L: 80), followed by removing columns with all NAs. Lastly, to achieve tidy data, we split the visual image selection and comprehension question into two columns so that a single observation is now based on each trial for each participant (e.g., pivot into a wider structure (L: 84)). Removal of columns in lines 86-88 is performed to make pivoting possible. Pivoting requires that rows do not have uniquely identifiable information outside the data columns being "widened" (This could also be achieved with the column argument of \inlineR{pivot\_wider}). 

\lstinputlisting[language=R, firstnumber=76]{scripts/chunk-Experimental Data: Clean and Tidy.R}

Additionally, we must load in a second data frame \inlineR{OSF\_data} (L: 94) from the original experiment. We do this for two reasons: 1) because our experiment only has the quadrants or the visual stimuli without the target, competitor, and distractor information, and 2) our experiment does not have \inlineR{SUBTLWF\_obj}, which is the log frequency of the object words needed much later in the statistical models.

\lstinputlisting[language=R, firstnumber=93]{scripts/chunk-OSF Data: Clean and Tidy.R}

In line 99, we filter the OSF data for experimental items and use a \inlineR{left\_join()} based on talker, condition, and the center visual image, which simultaneously pulls in the variables that we need and filters out nonce items (this step could be avoided by putting these variables in the original experimental spreadsheets). See figure \ref{fig:joins} for 

\lstinputlisting[language=R, firstnumber=98]{scripts/chunk-Behavioral Data: Join OSF and Experimental Data.R}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/joins.png}
    \caption{Joins act as filters, determining what data to include or exclude based on commonalities and differences between data frames. Solid portions refer to what is kept.}
    \label{fig:joins}
\end{figure}

Now that we have the variables we need from joining the datasets to create behavioral\_data using \inlineR{left\_join()}, we can create variables for the answers being correct and incorrect for our removal process. We will do this for both the item selection (L: 105) and the following comprehension question (L: 106).

\lstinputlisting[language=R, firstnumber=102]{scripts/chunk-Behavioral Data: Clean and Tidy.R}

Additionally, before beginning removal, we select criteria for removal. These numbers were created by us as the original study did not include this information, but we do suggest that future studies include such removal standards. We based our removal on three standard deviations away from the mean inaccuracy of participants/items separately.  

\lstinputlisting[language=R, firstnumber=108]{scripts/chunk-Behavioral Data: Removal Standards.R}

 We implemented these standards through a series of modular aggregating steps. That is, we aggregated the inaccuracies of participants by simply adding together incorrect items by participant and item for both item selection (L: 118-129) and comprehension question (L: 131-142), respectively. Then in line 144, individuals incorrect trials are removed. We end here with removing the incorrect trials to prepare for the eye-tracking data wrangling (L: 144:145).

\lstinputlisting[language=R, firstnumber=116]{scripts/chunk-Behavioral Data: Participant and Item Removal.R}

One important note about this removal is that the results would be different if you sequentially piped in the participant\_agg data frame as the base frame for item removal. This is because it would remove some incorrect items' scores simply based on the participants that were removed. We decided to pull in from the original behavioral\_data to avoid creating a bias in removal standards. 

\subsubsection{Eye-fixations, adjusted time, and frame rates}

In this last section of data removal, removal is done through an exploratory lens as there is little current reference for expected results for eye-fixations and frame rate in web-based eye-tracking. However, recent work has begun to fill this gap \parencite[see:][, for other pioneering work in web-based eye-tracking]{Vos_2017,Prystauka_Altmann_Rothman_2023}. Here, two questions guide us: How should eye-fixations be classified into quadrants in web-based eye-tracking? And, what quality of frame rate is needed to capture effects of predictive sentence processing? Like before, we start with filtering out participants from the previous data sets. Here, the retained participants (L: 118) and items (L: 131) from the previous step are used to define what we want to keep in behavioral data (L: 148-150) with the \inlineR{\%in\%} operator.

\lstinputlisting[language=R, firstnumber=147]{scripts/chunk-Behavioral Data: Removing with IN Operator.R}

Whereas the et\_data is much larger than the previous data frames, the same methods are used. Selection of data can be reduced to only the time, participant, and eye-fixations (L: 154-156), which is filtered by only usable fixation points where the variables are renamed (L: 158-159).

\lstinputlisting[language=R, firstnumber=153]{scripts/chunk-ET Data: Tidying and Filtering with an Inner Join.R}

Now that both behavioral\_data and et\_data are cleaned and tidy. Left join (L: 173) is used to create all\_data from our behavioral data and eye\_tracking data. This data frame now has all of the eye-tracking data and behavioral-task data from the entire experiment (L: 173-174). However, the data from the et\_data only includes unclassified eye-fixations. Specifically, it includes the x and y coordinates discussed in Act 1 without a link to the visual stimuli that are being viewed. A Shiny app was created to dynamically explore how eye-fixations are distributed with variable amounts of removal at four crucial time points: the beginning of the sentence (-400 ms), verb onset (0 ms), object onset, and selection of visual stimuli. The app also includes dynamically calculated data loss.  Figure \ref{fig:signal_noise} is a fixed version of visual of the fixation points from the app. Click \href{https://i4fzvw-adam-bramlett.shinyapps.io/Eye_Fixations_App/} {here} for the link to the Shiny app. In the discussion, implications of removal standards based on eye-fixation alone are considered and discussed as a signal detection problem.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/signal_noise.pdf}
    \caption{Quadrant locations and actual screen sizes are denoted with white lines}
    \label{fig:signal_noise}
\end{figure}

From lines 180-190, we create a classification system based on no inner-edge removal of the eye-fixations and partial removal of outer-edge eye-fixations (the code was created with inner removal in mind so that future researchers can simply adapt the distance variable L: 177, if desired). We use two types of control flow to first classify eye-fixations into quadrants and then create binary variables to link the quadrant to the visual stimuli. 
\inlineR{case\_when()} is used (L: 180-190) because of the multiple conditions and because \inlineR{Case\_when()} is only truth evaluating, meaning that it only provides a specific output in the case of something being true. For example, if we only want to classify images that are within a particular space and leave others blank, then non-binary classification like \inlineR{Case\_when()} is optimal. In contrast, if the outcomes of a classification is binary, then \inlineR{ifelse()} is an effective solution. For example, L: 192-200 makes a binary decision on whether an image being viewed is the same or different from the target (L: 193), competitors (L: 194-195), and distractor (L: 196), separately. While complexity of implementation may vary, logically, either can be used to achieve the same result in all cases with the use of operators and/or nesting.

\lstinputlisting[language=R, firstnumber=171]{scripts/chunk-ET Data: Localizing Visual Stimuli.R}

In addition to more variable eye-fixation, web-based eye-tracking also has variable frame rates. That is, the frame rates across trials between participants at any given point are variable, yet mostly consistent \parencite[][]{Vos_2017}{}{}. Further, the frame rates of each individual varies over trials. Figure \ref{fig:Participant_frame_rates} depicts a categorization of participants by median frame rate across trials. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/Participant_frame_rates.pdf}
    \caption{Mean frame rate is marked with dotted horizontal line}
    \label{fig:Participant_frame_rates}
\end{figure}
Like other recent web-based eye-tracking studies, our mean frame rate is ~20Hz (m:~22.17Hz, SD:11.61). Here, we remove the five participants with less than 5Hz median frame rates and create time bins by first creating a standard for removal in L: 378 and a binning size (L: 379). We then aggregate by participant, item, and condition (L: 381) in order to remove all participants that are below the predetermined median (L: 381-388). Next, time bins are created by normalizing the time range for each item (L: 389). Additionally, we subtracted 200 ms for human eye movements to occur (this step was not explicit in \textcite{Porretta_et_al_2020}, but we recommend future research to include this). Said another way, we center the time so that 0 is always the onset of the verb of interest. After normalizing, bins are created by dividing the time by the bin size, rounding, then multiplying by the bin size (L: 390), which is simply rounding items to the nearest bin size number.

\lstinputlisting[language=R, firstnumber=377]{scripts/chunk-All Data: Clean and Tidy.R}

Creating time bins is fundamentally discretizing a continuous scale. In any fixed set of eye-tracking data the grain size of the time scale has an inverse relationship to the amount of data in each time bin. Said another way, if you increase the bin size, you will have more data per bin, but less bins across time. As you will see in Act 3, many statistical analyses can bypass the binning procedure altogether by keeping time a continuous variable. Nevertheless, for analyses that do require time bins and for visualization alone, it is worth exploring whether specific bin sizes affect a researcher's ability to capture an effect. To do this, we created a second Shiny app that is depicted in figure \ref{fig:frame_rates_over_trial}, which allows the reader to explore the interactions between data removal based on participant median frame rates, changing bin sizes, and seeing output in the form of empirical logits.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/frame_rates_over_trial.pdf}
    \caption{Total looks per bin size. }
    \label{fig:frame_rates_over_trial}
\end{figure}

The final step before visualizing and doing statistical analysis is to do a final tidying. Like the first wrangling that we did, we create a tidy data frame by removing all binned data that is outside the time of interest -400ms and 800ms. Note this will actually only remove data points outside this range +/- the binning number. This is because we are using the rounded time variable to filter the data. 

Between the two data frames all\_data\_cleaned and all\_data\_tidy we have all of the behaviorial data ready for any analysis or exploration that can be done. Wrangling continues in analysis. However, the major wrangling is all complete. At this point, right\_join could be used to join data\_quest\_tidy to all\_data\_tidy or all\_data\_cleaned. However, this is not necessary, since all data frames are ready for adding in any variables. Wrangling continues in Act: 3. 

    


